{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Learning 2nd Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Question 1\n",
    " \n",
    " - Formula for the distribution is:\n",
    " > $$ l(\\mu, \\sigma^2, x_1, x_2, ... ,x_n ) = ln( (2\\pi\\sigma^2)^{-n/2} ) + ln(exp(-\\frac{1}{2\\sigma^2 \\sum\\limits_{i=1}^n (x_i - \\mu)^2})) $$\n",
    " \n",
    " if we derive the log estimate above by our unknown value $\\mu$ and then set that equal to $0$ we can get a maximum estimation for $\\mu$.\n",
    " \n",
    " - \n",
    " - Writing down log estimation _(log because the derivative of products would be quite difficult)_ from our sample: \n",
    " \n",
    " > $$ log(L(\\theta)) = log\\left(\\prod_{i=1}^{n} P(X_i|\\theta)\\right) = log\\left(\\frac{2\\theta}{3}\\right) + log\\left(\\frac{2\\theta}{3}\\right) + log\\left(\\frac{\\theta}{3}\\right) + log\\left(\\frac{\\theta}{3}\\right) + log\\left(\\frac{\\theta}{3}\\right) + log\\left(\\frac{2(1-\\theta)}{3}\\right) + log\\left(\\frac{2(1-\\theta)}{3}\\right) + log\\left(\\frac{2(1-\\theta)}{3}\\right) + log\\left(\\frac{(1-\\theta)}{3}\\right) + log\\left(\\frac{(1-\\theta)}{3}\\right) $$\n",
    " \n",
    " > $$ = 5log(\\theta) + 5log(1-\\theta) +\\, ...$$\n",
    " \n",
    " Deriving both sides with theta\n",
    " \n",
    " > $$ \\frac{dlog(L(\\theta))}{d\\theta} = 5\\frac{1}{\\theta} - 5\\frac{1}{1-\\theta} \\, = \\, 0 $$ <br>\n",
    " > $$ 10\\theta = 5 $$\n",
    " \n",
    " From equation above we get $ \\theta = 0,5 $\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    " - > $ P(content | not\\, rich, married, healthy) = \\frac{p(not\\, rich| content) * p(married|content) * p(healthy|content) * p(content)}{p(not\\, rich, married, healthy)} $\n",
    " <br>\n",
    " > $ = \\frac{\\frac{1}{27} \\times \\frac{2}{27} \\times \\frac{3}{27} \\times \\frac{12}{27}}{\\frac{5}{27} \\times \\frac{3}{27} \\times \\frac{3}{27}} = 0.059 = 5.9\\% $ \n",
    " \n",
    " - > $P(content | not\\, rich, married) = \\frac{(p(not\\, rich| content) * p(married|content) * p(healthy|content) + p(not\\, rich| content) * p(married|content) * p(not\\, healthy|content)) * p(content)}{p(not\\, rich, married, healthy) + p(not\\, rich, married, not\\, healthy)}$\n",
    " <br>\n",
    " > $= \\frac{(\\frac{1}{27} \\times \\frac{2}{27} \\times \\frac{3}{27} + \\frac{1}{27} \\times \\frac{2}{27} \\times \\frac{1}{27})  \\times \\frac{12}{27}}{\\frac{5}{27} \\times \\frac{3}{27} \\times \\frac{3}{27} + \\frac{5}{27} \\times \\frac{3}{27} \\times \\frac{5}{27}}$\n",
    " <br><br>\n",
    " > $=0.029 = 2.9\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Task given to us on this part is to develop a machine learning program using naive bayes algorith that can differentiate between fake and real news.\n",
    "For this we have given two datasets one being fake new headlines while other being real ones. We needed to implement naive bayes with bag of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My approach\n",
    "Overall idea was to parse the training data into 4 seperate bag of words; fake and real bags with unigram and bigram words. Then using the bags calculate the naive bayes for each test headline to check whether it is fake or real.\n",
    "I used multiple different methods to improve my accuracy most of which did not helped with such a small dataset.\n",
    "These improvements are: \n",
    "\n",
    "#### 1. Stopword removal\n",
    "For starters I removed stopwords from ___bags___. Idea was to remove commonly used words like these to increase accuracy. But sadly not only it did not helped a bit, it made the accuracy score even worse. I used the scikit-learn library to get english stopwords.\n",
    "\n",
    "#### 2. Token addition\n",
    "This one is super quirky. I added some kind of token to both at the beginning and the end of the both test and training headlines. The results were suprisingly positive, tokens actually improve score by average of __1%__. \n",
    "The issue is how to/where to add the tokens. <br>\n",
    "So far I tried few methods: <br>\n",
    " - Appending them directly to the line, without any space. This actually improved the unigram results by about __1.5%__ while it actually lowered the accuracy of bigrams by about __1.5%__.\n",
    " - Appending them to line with space in between the line. This improved both the bigram and unigram, benefitting the bigram more. Average improvement is about __1%__.\n",
    " - Apending with whitespace both before and after the token. This one is amazing. It combined the results of the previous two somehow. Bigram is improved about __1-1.5%__ while unigram improved about __1.7%__. I have no idea how this works, but it does work.\n",
    " \n",
    "I actually gone with second option. I also remove token from the bags after the parsing done since we were supposed to add token only to bigrams. Or so I heard.\n",
    "\n",
    "#### 3. TF-IDF\n",
    "I added this as last. It is by far the worst addition to the program. Since the data is small (both by document count and the lines itself) there is actually no reason for us to use IDF. But since we are asked, I did add it anyway.\n",
    "The formula I used for this is:\n",
    " > $ TF = log(\\frac{count(given\\, word)}{count(word\\, in\\, class)}) $ <br><br>\n",
    " > $ IDF = \\left|log(1 + \\frac{count(documents\\, having\\, word)}{count(document)})\\right| $ <br><br>\n",
    " > $ TF\\_IDF = TF \\times IDF $\n",
    " \n",
    "I tried two different approach with this since I wasn't sure what the intended ___document___ was. \n",
    "\n",
    " - I take the training files as documents, so the document count is ___2___. This one, well, changes nothing since the idf number is so small it has close to 0 impact(as top10 wise). The overall accuracy score dropped by about __9-10%__.\n",
    " - Take the each line as document. This idea, of course, faulty from start. If we take lines as document, since TF-IDF helps us to find what the document is about, what we gonna find/aim to find is what are the most common/influential word in the line instead of the whole class(fake/real). But for the lol's I tried it anyway. This time score dropped by about whopping __20%__. So overall, super bad idea.\n",
    " \n",
    "#### 4. Stem\n",
    "I didn't implement the stemming. That seems like a bad idea and as far as I heard it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Approach Part 2: Electric Boogaloo\n",
    "In this segment I will talk about implementation details. I put this into seperate segment so you can skip them.Actually go ahead and [skip](#Analysis-results).<br> No? Ok. <br>\n",
    "As the previous assignment I made use of object orientation. So I have seperate classes for most things. I extensively commented everything in the code, so I suggest you to check it out[.](#Analysis-results)\n",
    "\n",
    "#### Parsing\n",
    "The training data parsed into _Data_ objects. Each one of these objects holds two bags; one for bigram and other for unigram. It has methods to parse file and filter the bags. Nothing major. <br>\n",
    "The test data,since it is _special_, required a seperate class. So each test headline parsed into _Line_ objects, Line being subclass of the Data. It does not use file parsing, parsing done through a seperate function. Instead it parses line strings into bags. It also has special variables like accuracy scores and the prediction results. \n",
    "\n",
    "#### Data structure\n",
    "All the words held in bags. Namely BagOfWords class. The class is somewhat special and largest class in the program. But it is just a glorified dictionary wrapper. <br>\n",
    "Class itself methods to filter dictionary, methods that allows undirect access to it's core dictionary, methods that merges bags etc. You can see more of them in the, duh, code. <br>\n",
    "This class is more or less the core of the program since it handle all the data management.\n",
    "\n",
    "#### Calculations\n",
    "Bayes is, no suprise at this point, a seperate class. It has only one entrypoint; _fit_ method. Training data given to it at constructor and the test data is given to it in fit method. Also the bayes class is specialized to use Data and Line objects as data. <br>\n",
    "Since it uses the Data class instead of BagOfWords, that means it will calculate for both unigram and bigram together. The class calculates both fake and real seperately and then writes them Line objects. Then calls a method in Line object to predict if the given headline is fake or real.<br>\n",
    "Also calculation is done with ___Laplacian Smoothing___. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis results\n",
    "\n",
    "Below you will find the analysis scores for the naive bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A\n",
    "Below is the table of the results given. I used the scores calculated in the naive bayes while I made this list. Also I removed the common words from bags.\n",
    "\n",
    " - 1. ___List the 10 words whose presence most strongly predicts that the news is real.___\n",
    " \n",
    "|#|Unigram Word|Unigram Score|Bigram word|Bigram score|\n",
    "|-|-|-|-|-|\n",
    "|1|korea|-2.4851578613744634|north korea|-2.7516450275628213|\n",
    "|2|turnbull|-2.5943023307995317|travel ban|-2.9137255721756876|\n",
    "|3|travel|-2.603257173452458|ban __TOKEN__ |-3.105611098414601|\n",
    "|4|australia|-2.7793484325081392|korea __TOKEN__ |-3.156763620861982|\n",
    "|5|climate|-2.8865584021560076|trump travel|-3.3065259411953143|\n",
    "|6|paris|-2.962279116094126|malcolm turnbull|-3.360883603517907|\n",
    "|7|refugee|-2.9834684151640642|james comey|-3.39084682689535|\n",
    "|8|debate|-3.0803784281721205|trumps travel|-3.4230315102667515|\n",
    "|9|asia|-3.0803784281721205|comments __TOKEN__ |-3.4577936165259633|\n",
    "|10|congress|-3.108407151772364|wall st|-3.495582177415363|\n",
    "\n",
    "<center><b>TOKEN</b> is the string added before and after the lines.</center><br>\n",
    "\n",
    " - 2. ___List the 10 words whose absence most strongly predicts that the news is real.___\n",
    " \n",
    "|#|Unigram Word|Unigram Score|Bigram word|Bigram score|\n",
    "|-|-|-|-|-|\n",
    "|1|breaking|-2.875292825371008|__TOKEN__ comment|-3.076211189636061|\n",
    "|2|3|-3.0427839126647718|__TOKEN__  watch|-3.092005456819293|\n",
    "|3|woman|-3.1271047983648077|__TOKEN__  breaking|-3.180946540156074|\n",
    "|4|dr|-3.1940515879954208|i m|-3.292920299600006|\n",
    "|5|reason|-3.2318401488848205|wire __TOKEN__ |-3.444187974930655|\n",
    "|6|interview|-3.273232834043046|trump won|-3.444187974930655|\n",
    "|7|info|-3.273232834043046|voting for|-3.444187974930655|\n",
    "|8|my|-3.273232834043046|daily wire|-3.444187974930655|\n",
    "|9|homeless|-3.273232834043046|will win|-3.481976535820055|\n",
    "|10|d|-3.3189903246037207|breaking trump|-3.5233692209782803|\n",
    "\n",
    "\n",
    "<center> <b>TOKEN</b> is the string added before and after the lines.</center> <br>\n",
    "\n",
    " - 3. ___List the 10 words whose presence most strongly predicts that the news is fake.___ <br>\n",
    " To me it is same as the number 2. Since I removed duplicates if the word is in top 10 reals it is in least 10 fakes.\n",
    " - 4. ___List the 10 words whose absence most strongly predicts that the news is fake.___ <br>\n",
    " Also similar to number 1. (See answer above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B\n",
    " - 1. ___List the 10 non-stopwords that most strongly predict that the news is fake___\n",
    " \n",
    "|#|Stopwords|Score|\n",
    "|-|-|-|\n",
    "|1|my|-3.273232834043046|\n",
    "|2|these|-3.370142847051102|\n",
    "|3|any|-3.6711728427150834|\n",
    "|4|something|-3.7961115793233833|\n",
    "|5|towards|-3.7961115793233833|\n",
    "|6|am|-3.7961115793233833|\n",
    "|7|such|-3.9722028383790646|\n",
    "|8|found|-3.9722028383790646|\n",
    "|9|everywhere|-3.9722028383790646|\n",
    "|10|via|0|\n",
    " \n",
    " - 2. ___List the 10 non-stopwords that most strongly predict that the news is real___\n",
    " \n",
    "|#|Stopwords|Score|\n",
    "|-|-|-|\n",
    "|1|third|-3.682438419500083|\n",
    "|2|further|-3.807377156108383|\n",
    "|3|itself|0|\n",
    "|4|nine|0|\n",
    "|5|interest|0|\n",
    "|6|serious|0|\n",
    "|7|across|0|\n",
    "|8|detail|0|\n",
    "\n",
    "The reason why there is 8 of them is simply there is only that much stopword was in real data after cleaning up common stopwords in both real and fake data. <br>\n",
    "Reason why most of them 0 is the way I calculate scores for this list. I use naive bayes calculation for this. And since there is no such words in the test data the count of words comes as 0. So the result becomes ___0___. _If they are not in test data they cannot be used for the analysis since they are not used in prediction anyway._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C\n",
    "The idea for to remove stopwords they are very commonly used in most sentences.They do not bring any unique meaning to the sentence/headline. Since that's the case removing them might reduce the chance of overfitting and increase the overall accuracy for prediction. __BUT__ <br>\n",
    "While idea is solid, for our dataset the results says the otherwise. That means while they are commonly encountered in sentences they do indeed bring some meaning to it. From results we can say _it is possible to distinguish a fake headline from real one by looking at what stopwords used_.<br> By no means that would give us a accurate result but it can help with prediction. So removing them might not be a good idea. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D\n",
    "Here is the some of the accuracy scores I found. I didn't tried stemming.\n",
    "\n",
    "|#|Token|Stopword filter|TF_IDF|Unigram accuracy|Bigram accuracy|\n",
    "|-|-|-|-|-|-|\n",
    "|1|Yes(With space before and after)|No|No|86.91%|86.30%|\n",
    "|2|Yes(No spaces around)|No|No|86.71%|82.82%|\n",
    "|3|Yes(With space between line and token)|No|No|85.69%|85.07%|\n",
    "|4|Yes(With space between line and token)|Yes|No|85.48%|85.07%|\n",
    "|5|Yes(No spaces around)|Yes|No|84.05%|82.82%|\n",
    "|6|Yes(With space between line and token)|No|Yes|66.67%|67.69%|\n",
    "|7|Yes(With space between line and token)|Yes|Yes|65.85%|67.69%|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
